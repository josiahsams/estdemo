{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@63ded03c"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataframes\n",
    "![DataFrame](df1.png \"Spark DataFrames\")\n",
    "***\n",
    "\n",
    "\n",
    "## Spark ML Pipeline\n",
    "![pipeline](ml-pipeline.png)\n",
    "****\n",
    "\n",
    "\n",
    "### WorkFlow\n",
    "![dataframe](dataframe.png)\n",
    "****\n",
    "\n",
    "\n",
    "### Cross Validator for Best Model\n",
    "![cross](crossvalidator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.ibm.snap.ml.{SnapLogisticRegression=>LogisticRegression}\n",
       "import com.ibm.snap.ml.{SnapLogisticRegressionModel=>LogisticRegressionModel}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.feature.{Bucketizer, OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler}\n",
    "import com.ibm.snap.ml.{SnapLogisticRegression => LogisticRegression}\n",
    "import com.ibm.snap.ml.{SnapLogisticRegressionModel => LogisticRegressionModel}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n",
       "schema = StructType(StructField(yr,IntegerType,true), StructField(mon,IntegerType,true), StructField(dofM,IntegerType,true), StructField(dofW,IntegerType,true), StructField(date,StringType,true), StructField(carrier,StringType,true), StructField(origin,StringType,true), StructField(dest,StringType,true), StructField(crsdeptime,IntegerType,true), StructField(deptime,StringType,true), StructField(depdelay,DoubleType,true), StructField(crsarrtime,IntegerType,true), StructField(arrtime,StringType,true), StructField(arrdelay,DoubleType,true), StructField(canc,DoubleType,true), StructField(crselapsedtime,DoubleType,true), StructField(elapsed,DoubleType,true), StructField(air,DoubleType,true), Struc...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(yr,IntegerType,true), StructField(mon,IntegerType,true), StructField(dofM,IntegerType,true), StructField(dofW,IntegerType,true), StructField(date,StringType,true), StructField(carrier,StringType,true), StructField(origin,StringType,true), StructField(dest,StringType,true), StructField(crsdeptime,IntegerType,true), StructField(deptime,StringType,true), StructField(depdelay,DoubleType,true), StructField(crsarrtime,IntegerType,true), StructField(arrtime,StringType,true), StructField(arrdelay,DoubleType,true), StructField(canc,DoubleType,true), StructField(crselapsedtime,DoubleType,true), StructField(elapsed,DoubleType,true), StructField(air,DoubleType,true), Struc..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "case class Flight(yr: Integer, mon: Integer, dofM: Integer, dofW: Integer, date: String, carrier: String, \n",
    "                  origin: String, dest: String, crsdeptime:String, deptime:String, depdelay:Double, \n",
    "                  crsarrtime:String, arrtime: String, arrdelay: Double, crselapsedtime: Double, \n",
    "                  elapsed: Double, air: Double, dist: Double, dummy: String) extends Serializable\n",
    "\n",
    "  val schema = StructType(Array(\n",
    "    StructField(\"yr\", IntegerType, true),\n",
    "    StructField(\"mon\", IntegerType, true),\n",
    "    StructField(\"dofM\", IntegerType, true),\n",
    "    StructField(\"dofW\", IntegerType, true),\n",
    "    StructField(\"date\", StringType, true),\n",
    "    StructField(\"carrier\", StringType, true),\n",
    "    StructField(\"origin\", StringType, true),\n",
    "    StructField(\"dest\", StringType, true),\n",
    "    StructField(\"crsdeptime\", IntegerType, true),\n",
    "    StructField(\"deptime\", StringType, true),  \n",
    "    StructField(\"depdelay\", DoubleType, true),\n",
    "    StructField(\"crsarrtime\", IntegerType, true),\n",
    "    StructField(\"arrtime\", StringType, true),    \n",
    "    StructField(\"arrdelay\", DoubleType, true),\n",
    "    StructField(\"canc\", DoubleType, true),  \n",
    "    StructField(\"crselapsedtime\", DoubleType, true),\n",
    "    StructField(\"elapsed\", DoubleType, true),  \n",
    "    StructField(\"air\", DoubleType, true),    \n",
    "    StructField(\"dist\", DoubleType, true),\n",
    "    StructField(\"dummy\", StringType, true),\n",
    "    StructField(\"crsdephour\", IntegerType, true)\n",
    "  ))\n",
    "\n",
    "// StructField(\"crsdephour\", IntegerType, true),crsdeptime, crsarrtime, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toInt: org.apache.spark.sql.expressions.UserDefinedFunction\n",
       "toDouble: org.apache.spark.sql.expressions.UserDefinedFunction\n",
       "getHour: org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toInt = udf {(str: String) => {\n",
    "  str.toInt\n",
    "}}\n",
    "\n",
    "def toDouble = udf {(str: String) => {\n",
    "  if (str == \"\") {\n",
    "        0.0\n",
    "  } else {\n",
    "      str.toDouble\n",
    "  }\n",
    "}}\n",
    "def getHour = udf {(str: String) => {\n",
    "    if (str == \"\") {\n",
    "        0\n",
    "    } else {\n",
    "       str.toInt / 100 \n",
    "    }\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames = List(yr, mon, dofM, dofW, date, carrier, origin, dest, crsdeptime, deptime, depdelay, crsarrtime, arrtime, arrdelay, canc, crselapsedtime, elapsed, air, dist, dummy)\n",
       "datafilesDF = [yr: int, mon: int ... 19 more fields]\n",
       "flightDS = [yr: int, mon: int ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 19 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = Seq(\"yr\", \"mon\", \"dofM\", \"dofW\", \"date\", \"carrier\", \"origin\", \"dest\", \"crsdeptime\", \"deptime\", \n",
    "                   \"depdelay\", \"crsarrtime\", \"arrtime\", \"arrdelay\", \"canc\", \"crselapsedtime\",\"elapsed\", \"air\", \"dist\", \"dummy\")\n",
    "\n",
    "\n",
    "\n",
    "val datafilesDF = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "        .load(\"./data\").toDF(colNames :_* )\n",
    "        .na.drop(Seq(\"yr\", \"mon\", \"dofM\", \"dofW\", \"depdelay\", \"crsarrtime\", \"crsdeptime\", \"crselapsedtime\", \"air\", \"dist\"))\n",
    "        .withColumn(\"yr\", toInt($\"yr\"))\n",
    "        .withColumn(\"mon\", toInt($\"mon\"))\n",
    "        .withColumn(\"dofM\", toInt($\"dofM\"))\n",
    "        .withColumn(\"dofW\", toInt($\"dofW\"))\n",
    "        .withColumn(\"dofW\", toInt($\"dofW\"))\n",
    "        .withColumn(\"depdelay\", toDouble($\"depdelay\"))\n",
    "        .withColumn(\"crsarrtime\", toInt($\"crsarrtime\"))\n",
    "        .withColumn(\"crsdeptime\", toInt($\"crsdeptime\"))\n",
    "        .withColumn(\"arrdelay\", toDouble($\"arrdelay\"))\n",
    "        .withColumn(\"canc\", toDouble($\"canc\"))\n",
    "        .withColumn(\"crselapsedtime\", toDouble($\"crselapsedtime\"))\n",
    "        .withColumn(\"elapsed\", toDouble($\"elapsed\"))\n",
    "        .withColumn(\"air\", toDouble($\"air\"))\n",
    "        .withColumn(\"dist\", toDouble($\"dist\"))\n",
    "        .withColumn(\"crsdephour\", getHour($\"deptime\"))\n",
    "\n",
    "val flightDS = datafilesDF.as[Flight].cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+----+----------+-------+------------+---------------+----------+-------+--------+----------+-------+--------+----+--------------+-------+-----+------+-----+----------+\n",
      "|  yr|mon|dofM|dofW|      date|carrier|      origin|           dest|crsdeptime|deptime|depdelay|crsarrtime|arrtime|arrdelay|canc|crselapsedtime|elapsed|  air|  dist|dummy|crsdephour|\n",
      "+----+---+----+----+----------+-------+------------+---------------+----------+-------+--------+----------+-------+--------+----+--------------+-------+-----+------+-----+----------+\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|Los Angeles, CA|      2110|   2159|    49.0|      2230|   2306|    36.0| 0.0|          80.0|   67.0| 52.0| 308.0| null|        21|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|Los Angeles, CA|      1445|   1440|    -5.0|      1605|   1559|    -6.0| 0.0|          80.0|   79.0| 51.0| 308.0| null|        14|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Orlando, FL|      1305|   1318|    13.0|      2100|   2115|    15.0| 0.0|         295.0|  297.0|278.0|2421.0| null|        13|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Chicago, IL|      1415|   1413|    -2.0|      2015|   2008|    -7.0| 0.0|         240.0|  235.0|220.0|1838.0| null|        14|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Chicago, IL|       650|   0647|    -3.0|      1255|   1254|    -1.0| 0.0|         245.0|  247.0|234.0|1838.0| null|         6|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Ontario, CA|      1930|   1930|     0.0|      2040|   2037|    -3.0| 0.0|          70.0|   67.0| 52.0| 333.0| null|        19|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Ontario, CA|      1615|   1624|     9.0|      1725|   1729|     4.0| 0.0|          70.0|   65.0| 54.0| 333.0| null|        16|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Ontario, CA|       755|   0748|    -7.0|       905|   0854|   -11.0| 0.0|          70.0|   66.0| 54.0| 333.0| null|         7|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Ontario, CA|      1020|   1015|    -5.0|      1130|   1126|    -4.0| 0.0|          70.0|   71.0| 51.0| 333.0| null|        10|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      1730|   1726|    -4.0|      1915|   1911|    -4.0| 0.0|         105.0|  105.0| 90.0| 569.0| null|        17|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      1440|   1441|     1.0|      1625|   1623|    -2.0| 0.0|         105.0|  102.0| 91.0| 569.0| null|        14|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      1950|   2127|    97.0|      2140|   2312|    92.0| 0.0|         110.0|  105.0| 91.0| 569.0| null|        21|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      1840|   1849|     9.0|      2025|   2039|    14.0| 0.0|         105.0|  110.0| 92.0| 569.0| null|        18|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|       840|   0837|    -3.0|      1025|   1025|     0.0| 0.0|         105.0|  108.0| 91.0| 569.0| null|         8|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      1125|   1123|    -2.0|      1310|   1303|    -7.0| 0.0|         105.0|  100.0| 88.0| 569.0| null|        11|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|   Portland, OR|      2145|   2146|     1.0|      2335|   2334|    -1.0| 0.0|         110.0|  108.0| 92.0| 569.0| null|        21|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Phoenix, AZ|      2030|   2119|    49.0|      2315|   2355|    40.0| 0.0|         105.0|   96.0| 85.0| 621.0| null|        21|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Phoenix, AZ|      1345|   1346|     1.0|      1630|   1623|    -7.0| 0.0|         105.0|   97.0| 84.0| 621.0| null|        13|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Phoenix, AZ|      1050|   1048|    -2.0|      1335|   1328|    -7.0| 0.0|         105.0|  100.0| 85.0| 621.0| null|        10|\n",
      "|2018| 12|  10|   1|2018-12-10|     WN|San Jose, CA|    Phoenix, AZ|       645|   0653|     8.0|       935|   0934|    -1.0| 0.0|         110.0|  101.0| 83.0| 621.0| null|         6|\n",
      "+----+---+----+----+----------+-------+------------+---------------+----------+-------+--------+----------+-------+--------+----+--------------+-------+-----+------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightDS.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2351068"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightDS.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- yr: integer (nullable = false)\n",
      " |-- mon: integer (nullable = false)\n",
      " |-- dofM: integer (nullable = false)\n",
      " |-- dofW: integer (nullable = false)\n",
      " |-- date: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- crsdeptime: integer (nullable = false)\n",
      " |-- deptime: string (nullable = true)\n",
      " |-- depdelay: double (nullable = false)\n",
      " |-- crsarrtime: integer (nullable = false)\n",
      " |-- arrtime: string (nullable = true)\n",
      " |-- arrdelay: double (nullable = false)\n",
      " |-- canc: double (nullable = false)\n",
      " |-- crselapsedtime: double (nullable = false)\n",
      " |-- elapsed: double (nullable = false)\n",
      " |-- air: double (nullable = false)\n",
      " |-- dist: double (nullable = false)\n",
      " |-- dummy: string (nullable = true)\n",
      " |-- crsdephour: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightDS.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDS.createOrReplaceTempView(\"flights\")\n",
    "spark.catalog.cacheTable(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depdelay = MapPartitionsRDD[30] at rdd at <console>:56\n",
       "arrdelay = MapPartitionsRDD[35] at rdd at <console>:57\n",
       "correlation = 0.9543348573117445\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9543348573117445"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics\n",
    "val depdelay = flightDS.select(\"depdelay\").map{row:Row => row.getAs[Double](\"depdelay\")}.rdd\n",
    "val arrdelay = flightDS.select( \"arrdelay\").map{row:Row => row.getAs[Double](\"arrdelay\")}.rdd\n",
    "val correlation = Statistics.corr(depdelay,arrdelay, \"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|             dist|    crselapsedtime|          depdelay|          arrdelay|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|          2351068|           2351068|           2351068|           2351068|\n",
      "|   mean|796.5423390561226|140.32010771275012| 8.128869943361911|3.5410962166981133|\n",
      "| stddev|595.6734193002211| 72.78377685827463|42.188337576537386| 44.54053869542065|\n",
      "|    min|             31.0|             -99.0|            -122.0|            -120.0|\n",
      "|    max|           4983.0|             703.0|            2109.0|            2153.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightDS.describe(\"dist\", \"crselapsedtime\",\"depdelay\", \"arrdelay\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+---------------...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-------+--------------------+--------------------+--------+----------+------+----+\n",
       "|carrier|              origin|                dest|depdelay|crsdephour|  dist|dofW|\n",
       "+-------+--------------------+--------------------+--------+----------+------+----+\n",
       "|     AA|        Hartford, CT|    Philadelphia, PA|  2109.0|        19| 196.0|   7|\n",
       "|     OO|Bristol/Johnson C...|         Atlanta, GA|  2098.0|         0| 227.0|   7|\n",
       "|     AA|  Raleigh/Durham, NC|Dallas/Fort Worth...|  1822.0|        18|1061.0|   6|\n",
       "|     YV|        Columbus, OH|         Houston, TX|  1789.0|        17| 986.0|   4|\n",
       "|     AA|          Austin, TX|    Philadelphia, PA|  1787.0|        13|1430.0|   7|\n",
       "+-------+--------------------+--------------------+--------+----------+------+----+\n",
       "\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "select carrier,origin, dest, depdelay,crsdephour, dist, dofW\n",
    "from flights \n",
    "order by depdelay desc limit 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|delayed|  count|\n",
      "+-------+-------+\n",
      "|    0.0|2158903|\n",
      "|    1.0| 192165|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "delaybucketizer = bucketizer_7a358c820f3a\n",
       "flightDS4 = [yr: int, mon: int ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 20 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val delaybucketizer = new Bucketizer().setInputCol(\"depdelay\")\n",
    "  .setOutputCol(\"delayed\").setSplits(Array(Double.NegativeInfinity,40.0,Double.PositiveInfinity))\n",
    "\n",
    "val flightDS4= delaybucketizer.transform(flightDS)\n",
    "\n",
    "flightDS4.groupBy(\"delayed\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|delayed| count|\n",
      "+-------+------+\n",
      "|    0.0|647139|\n",
      "|    1.0|192165|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractions = Map(0.0 -> 0.3, 1.0 -> 1.0)\n",
       "flightDS5 = [yr: int, mon: int ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 20 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractions = Map(0.0 -> .3, 1.0->1.0)\n",
    "val flightDS5 = flightDS4.stat.sampleBy(\"delayed\", fractions, 36L)\n",
    "flightDS5.groupBy(\"delayed\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalColumns = Array(carrier, origin, dest, dofW)\n",
       "stringIndexers = Array(strIdx_2878ba18a705, strIdx_8e21cd7975ff, strIdx_f8adc026ba71, strIdx_e1d915b03ce0)\n",
       "encoders = Array(oneHot_0ac4bb0bee21, oneHot_9404dd02b4bf, oneHot_c55552a9bd98, oneHot_77a698486db2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(oneHot_0ac4bb0bee21, oneHot_9404dd02b4bf, oneHot_c55552a9bd98, oneHot_77a698486db2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// categorical Column names\n",
    "val categoricalColumns = Array( \"carrier\", \"origin\", \"dest\", \"dofW\")\n",
    "\n",
    "\n",
    "// String Indexers will encode string categorial columns\n",
    "// into a column numeric indices\n",
    "val stringIndexers = categoricalColumns.map { colName =>\n",
    "      new StringIndexer()\n",
    "        .setInputCol(colName)\n",
    "        .setOutputCol(colName + \"Indexed\")\n",
    "        .setHandleInvalid(\"keep\")\n",
    "}\n",
    "\n",
    "\n",
    "//OneHotEncoders map number indices column to column of binary vectors\n",
    "val encoders = categoricalColumns.map { colName =>\n",
    "      new OneHotEncoder()\n",
    "        .setInputCol(colName+\"Indexed\")\n",
    "        .setOutputCol(colName + \"Enc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labeler = bucketizer_6e3a952d406a\n",
       "featureCols = Array(carrierEnc, destEnc, originEnc, dofWEnc, crsdephour, crselapsedtime, crsarrtime, crsdeptime, dist)\n",
       "assembler = vecAssembler_9201fd671880\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_9201fd671880"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labeler = new Bucketizer().setInputCol(\"depdelay\")\n",
    "   .setOutputCol(\"label\")\n",
    "   .setSplits(Array( Double.NegativeInfinity, 40.0, Double.PositiveInfinity))\n",
    "\n",
    "val featureCols = Array( \"carrierEnc\", \"destEnc\", \"originEnc\", \n",
    "   \"dofWEnc\",\"crsdephour\",\"crselapsedtime\",\"crsarrtime\",\"crsdeptime\",\"dist\")\n",
    "\n",
    "//put features into a feature vector column   \n",
    "val assembler = new VectorAssembler()\n",
    "   .setInputCols(featureCols)\n",
    "   .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline with feature transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps = Array(strIdx_2878ba18a705, strIdx_8e21cd7975ff, strIdx_f8adc026ba71, strIdx_e1d915b03ce0, oneHot_0ac4bb0bee21, oneHot_9404dd02b4bf, oneHot_c55552a9bd98, oneHot_77a698486db2, bucketizer_6e3a952d406a, vecAssembler_9201fd671880)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "feature_pipeline: org.apache.spark.m...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(strIdx_2878ba18a705, strIdx_8e21cd7975ff, strIdx_f8adc026ba71, strIdx_e1d915b03ce0, oneHot_0ac4bb0bee21, oneHot_9404dd02b4bf, oneHot_c55552a9bd98, oneHot_77a698486db2, bucketizer_6e3a952d406a, vecAssembler_9201fd671880)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val steps = stringIndexers ++ encoders  ++  Array(labeler, assembler)\n",
    "val feature_pipeline = new Pipeline().setStages(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featuresTransformer = pipeline_3dd211133730\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_3dd211133730"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featuresTransformer = feature_pipeline.fit(flightDS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(721,[0,24,394,71...|\n",
      "|(721,[0,17,394,71...|\n",
      "|(721,[0,78,394,71...|\n",
      "|(721,[0,47,394,71...|\n",
      "|(721,[0,47,394,71...|\n",
      "|(721,[0,47,394,71...|\n",
      "|(721,[0,27,394,71...|\n",
      "|(721,[0,27,394,71...|\n",
      "|(721,[0,27,394,71...|\n",
      "|(721,[0,81,394,71...|\n",
      "|(721,[0,81,394,71...|\n",
      "|(721,[0,39,394,71...|\n",
      "|(721,[0,39,394,71...|\n",
      "|(721,[0,39,394,71...|\n",
      "|(721,[0,39,394,71...|\n",
      "|(721,[0,34,394,71...|\n",
      "|(721,[0,60,394,71...|\n",
      "|(721,[0,60,394,71...|\n",
      "|(721,[0,44,394,71...|\n",
      "|(721,[0,84,394,71...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresTransformer.transform(flightDS5).select(\"features\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = SnapMLLogisticRegression_0a01582789b0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import com.ibm.snap.ml.{SnapLogisticRegression=>LogisticRegression}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SnapMLLogisticRegression_0a01582789b0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.ibm.snap.ml.{SnapLogisticRegression => LogisticRegression}\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_pipeline = pipeline_6e6116135751\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_6e6116135751"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_pipeline = new Pipeline().setStages(steps ++ Array(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tSnapMLLogisticRegression_0a01582789b0-balanced: true,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-maxIter: 10,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-regParam: 0.001\n",
       "}, {\n",
       "\tSnapMLLogisticRegression_0a01582789b0-balanced: false,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-maxIter: 10,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-regParam: 0.001\n",
       "}, {\n",
       "\tSnapMLLogisticRegression_0a01582789b0-balanced: true,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-maxIter: 10,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-regParam: 0.1\n",
       "}, {\n",
       "\tSnapMLLogisticRegression_0a01582789b0-balanced: false,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-maxIter: 10,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-regParam: 0.1\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEval...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array( 0.001, 0.1))\n",
    "    .addGrid(lr.balanced, Array(true, false))\n",
    "    .addGrid(lr.maxIter, Array(10))\n",
    "    .build()\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")   \n",
    "\n",
    "// Set up 3-fold cross validation with paramGrid\n",
    " val crossval = new CrossValidator().setEstimator(training_pipeline)\n",
    "      .setEvaluator (evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid).setNumFolds(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cross Validator Estimator to fit the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n",
      "[Info] Warning : tolerance 1.000000e-03 NOT attained after 10 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cvModel = cv_fc7c65bd2368\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_fc7c65bd2368"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = crossval.fit(flightDS5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-fold Cross Validation\n",
    "![cross1](cross.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel = SnapMLLogisticRegression_0a01582789b0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SnapMLLogisticRegression_0a01582789b0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = cvModel.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel]\n",
    "            .stages.last\n",
    "            .asInstanceOf[LogisticRegressionModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "balanced: If set to ‘False’, all classes will have weight 1. (default: false, current: false)\n",
       "dual: Dual or Primal formulation. Recommendation: if n_samples > n_features use dual=True. (default: true)\n",
       "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
       "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
       "featuresCol: features column name (default: features, current: features)\n",
       "gpuMemLimit: Limit of the GPU memory. If set to the default value 0, the maximum possible memory is used. (default: 0)\n",
       "labelCol: label column name (default: label, current: label)\n",
       "...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrModel.explainParams() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tSnapMLLogisticRegression_0a01582789b0-balanced: false,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-dual: true,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-elasticNetParam: 0.0,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-family: auto,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-featuresCol: features,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-gpuMemLimit: 0,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-labelCol: label,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-maxIter: 10,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-nthreads: 1,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-predictionCol: prediction,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-probabilityCol: probability,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-rawPredictionCol: rawPrediction,\n",
       "\tSnapMLLogisticRegression_0a01582789b0-regPara...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testfilesDF = [yr: int, mon: int ... 19 more fields]\n",
       "testDS = [yr: int, mon: int ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 19 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testfilesDF = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "        .load(\"./test\").toDF(colNames :_* )\n",
    "        .na.drop(Seq(\"yr\", \"mon\", \"dofM\", \"dofW\", \"depdelay\", \"crsarrtime\", \"crsdeptime\", \"crselapsedtime\", \"air\", \"dist\"))\n",
    "        .withColumn(\"yr\", toInt($\"yr\"))\n",
    "        .withColumn(\"mon\", toInt($\"mon\"))\n",
    "        .withColumn(\"dofM\", toInt($\"dofM\"))\n",
    "        .withColumn(\"dofW\", toInt($\"dofW\"))\n",
    "        .withColumn(\"dofW\", toInt($\"dofW\"))\n",
    "        .withColumn(\"depdelay\", toDouble($\"depdelay\"))\n",
    "        .withColumn(\"crsarrtime\", toInt($\"crsarrtime\"))\n",
    "        .withColumn(\"crsdeptime\", toInt($\"crsdeptime\"))\n",
    "        .withColumn(\"arrdelay\", toDouble($\"arrdelay\"))\n",
    "        .withColumn(\"canc\", toDouble($\"canc\"))\n",
    "        .withColumn(\"crselapsedtime\", toDouble($\"crselapsedtime\"))\n",
    "        .withColumn(\"elapsed\", toDouble($\"elapsed\"))\n",
    "        .withColumn(\"air\", toDouble($\"air\"))\n",
    "        .withColumn(\"dist\", toDouble($\"dist\"))\n",
    "        .withColumn(\"crsdephour\", getHour($\"deptime\"))\n",
    "\n",
    "val testDS = testfilesDF.as[Flight].cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|delayed| count|\n",
      "+-------+------+\n",
      "|    0.0|530054|\n",
      "|    1.0| 46196|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testDS4 = [yr: int, mon: int ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 20 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDS4 = delaybucketizer.transform(testDS)\n",
    "\n",
    "testDS4.groupBy(\"delayed\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|delayed|count|\n",
      "+-------+-----+\n",
      "|    0.0|53027|\n",
      "|    1.0|46196|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractions = Map(0.0 -> 0.1, 1.0 -> 1.0)\n",
       "testDS5 = [yr: int, mon: int ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 20 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractions = Map(0.0 -> .1, 1.0->1.0)\n",
    "val testDS5 = testDS4.stat.sampleBy(\"delayed\", fractions, 36L)\n",
    "testDS5.groupBy(\"delayed\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testDS6 = [yr: int, mon: int ... 30 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 30 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDS6 = featuresTransformer.transform(testDS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(721,[10,20,393,7...|\n",
      "|(721,[10,34,449,7...|\n",
      "|(721,[10,34,393,7...|\n",
      "|(721,[10,100,380,...|\n",
      "|(721,[10,58,372,7...|\n",
      "|(721,[10,44,380,7...|\n",
      "|(721,[10,34,413,7...|\n",
      "|(721,[10,39,375,7...|\n",
      "|(721,[10,42,380,7...|\n",
      "|(721,[10,39,376,7...|\n",
      "|(721,[10,48,376,7...|\n",
      "|(721,[10,28,394,7...|\n",
      "|(721,[10,77,393,7...|\n",
      "|(721,[10,77,370,7...|\n",
      "|(721,[10,34,447,7...|\n",
      "|(721,[10,101,394,...|\n",
      "|(721,[10,101,380,...|\n",
      "|(721,[10,51,421,7...|\n",
      "|(721,[10,47,421,7...|\n",
      "|(721,[10,26,370,7...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDS6.select(\"features\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions from Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions = [yr: int, mon: int ... 33 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[yr: int, mon: int ... 33 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = cvModel.transform(testDS5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the predictions accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy = 0.9054957016014432\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9054957016014432"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lp = [label: double, prediction: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, prediction: double]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions.select(\"label\", \"prediction\")\n",
    "lp.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counttotal = 99223\n",
       "label0count = 53027\n",
       "pred0count = 52336\n",
       "label1count = 46196\n",
       "pred1count = 46887\n",
       "correct = 89846\n",
       "wrong = 9377\n",
       "ratioWrong = 0.09450429839855679\n",
       "ratioCorrect = 0.9054957016014432\n",
       "truep = 0.4836882577628171\n",
       "truen = 0.4218074438386261\n",
       "falsep = 0.04377009362748556\n",
       "falsen = 0.050734204771071226\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.050734204771071226"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counttotal = predictions.count()\n",
    "val label0count  = lp.filter($\"label\" === 0.0).count()\n",
    "val pred0count = lp.filter($\"prediction\" === 0.0).count()\n",
    "val label1count = lp.filter($\"label\" === 1.0).count()\n",
    "val pred1count = lp.filter($\"prediction\" === 1.0).count()\n",
    "\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "val ratioWrong = wrong.toDouble / counttotal.toDouble\n",
    "val ratioCorrect = correct.toDouble / counttotal.toDouble\n",
    "val truep = lp.filter($\"prediction\" === 0.0)\n",
    "    .filter($\"label\" === $\"prediction\").count() / counttotal.toDouble\n",
    "val truen = lp.filter($\"prediction\" === 1.0)\n",
    "    .filter($\"label\" === $\"prediction\").count() / counttotal.toDouble\n",
    "val falsep = lp.filter($\"prediction\" === 0.0)\n",
    "    .filter(not($\"label\" === $\"prediction\")).count() / counttotal.toDouble\n",
    "val falsen = lp.filter($\"prediction\" === 1.0)\n",
    "    .filter(not($\"label\" === $\"prediction\")).count() / counttotal.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.write.overwrite().save(\"./FlightModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameCVModel = cv_fc7c65bd2368\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_fc7c65bd2368"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameCVModel = CrossValidatorModel.load(\"./FlightModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
